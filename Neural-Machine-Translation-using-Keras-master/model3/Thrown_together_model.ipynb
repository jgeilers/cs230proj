{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://github.com/vibhor98/Neural-Machine-Translation-using-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2315\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3686\n",
      "German Max Length: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 10, 256)           943616    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_8 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 5, 2315)           594955    \n",
      "=================================================================\n",
      "Total params: 2,589,195\n",
      "Trainable params: 2,589,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /anaconda3/envs/nlu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 26s - loss: 4.3379 - val_loss: 3.5642\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.56416, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 22s - loss: 3.4171 - val_loss: 3.4144\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.56416 to 3.41444, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 22s - loss: 3.2732 - val_loss: 3.3419\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.41444 to 3.34194, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 22s - loss: 3.1473 - val_loss: 3.2334\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.34194 to 3.23342, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 22s - loss: 3.0011 - val_loss: 3.1352\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.23342 to 3.13522, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 22s - loss: 2.8305 - val_loss: 2.9773\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.13522 to 2.97735, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 22s - loss: 2.6648 - val_loss: 2.8639\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.97735 to 2.86395, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 22s - loss: 2.5061 - val_loss: 2.7494\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.86395 to 2.74945, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 22s - loss: 2.3502 - val_loss: 2.6577\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.74945 to 2.65769, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 22s - loss: 2.2114 - val_loss: 2.5704\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.65769 to 2.57044, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 22s - loss: 2.0793 - val_loss: 2.4927\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.57044 to 2.49272, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 22s - loss: 1.9492 - val_loss: 2.4435\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.49272 to 2.44349, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 22s - loss: 1.8226 - val_loss: 2.3692\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.44349 to 2.36922, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 22s - loss: 1.7013 - val_loss: 2.3138\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.36922 to 2.31377, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 27s - loss: 1.5839 - val_loss: 2.2543\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.31377 to 2.25428, saving model to model.h5\n",
      "Epoch 16/30\n",
      " - 27s - loss: 1.4762 - val_loss: 2.2161\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.25428 to 2.21613, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 28s - loss: 1.3743 - val_loss: 2.1834\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.21613 to 2.18340, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 30s - loss: 1.2785 - val_loss: 2.1444\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.18340 to 2.14441, saving model to model.h5\n",
      "Epoch 19/30\n",
      " - 29s - loss: 1.1823 - val_loss: 2.1224\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.14441 to 2.12239, saving model to model.h5\n",
      "Epoch 20/30\n",
      " - 32s - loss: 1.0905 - val_loss: 2.0811\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.12239 to 2.08108, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 28s - loss: 1.0076 - val_loss: 2.0644\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.08108 to 2.06437, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 27s - loss: 0.9288 - val_loss: 2.0576\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.06437 to 2.05760, saving model to model.h5\n",
      "Epoch 23/30\n",
      " - 26s - loss: 0.8557 - val_loss: 2.0439\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.05760 to 2.04394, saving model to model.h5\n",
      "Epoch 24/30\n",
      " - 26s - loss: 0.7832 - val_loss: 2.0444\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.04394\n",
      "Epoch 25/30\n",
      " - 30s - loss: 0.7181 - val_loss: 2.0153\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.04394 to 2.01533, saving model to model.h5\n",
      "Epoch 26/30\n",
      " - 23s - loss: 0.6530 - val_loss: 2.0208\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.01533\n",
      "Epoch 27/30\n",
      " - 22s - loss: 0.5944 - val_loss: 1.9982\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.01533 to 1.99822, saving model to model.h5\n",
      "Epoch 28/30\n",
      " - 22s - loss: 0.5442 - val_loss: 1.9969\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.99822 to 1.99692, saving model to model.h5\n",
      "Epoch 29/30\n",
      " - 22s - loss: 0.4981 - val_loss: 1.9873\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.99692 to 1.98730, saving model to model.h5\n",
      "Epoch 30/30\n",
      " - 22s - loss: 0.4569 - val_loss: 2.0028\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.98730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3756f550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Encoder-Decoder model to represent word embeddings and finally\n",
    "# save the trained model as 'model.h5'\n",
    "\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on trained examples\n",
      "src=[bleib bei uns], target=[stay with us], predicted=[stay with us]\n",
      "src=[sie will ihn], target=[she wants him], predicted=[she wants him]\n",
      "src=[du bist stark], target=[youre strong], predicted=[youre strong]\n",
      "src=[untersuchen sie das], target=[examine this], predicted=[examine this]\n",
      "src=[hier ist meine karte], target=[heres my card], predicted=[heres my card]\n",
      "src=[tom stie auf], target=[tom burped], predicted=[tom burped]\n",
      "src=[das ist kein witz], target=[it is no joke], predicted=[its is a joke]\n",
      "src=[tom ist ein spion], target=[tom is a spy], predicted=[tom is a spy]\n",
      "src=[ich bin ein teenager], target=[im a teenager], predicted=[im a teenager]\n",
      "src=[ich bin nicht verruckt], target=[im not crazy], predicted=[im not crazy]\n",
      "Bleu-1: 0.073973\n",
      "Bleu-2: 0.215524\n",
      "Bleu-3: 0.294934\n",
      "Bleu-4: 0.261574\n",
      "Testing on test examples\n",
      "src=[viele bewundern ihn], target=[many admire him], predicted=[look at it]\n",
      "src=[es ist nicht wahr], target=[it isnt true], predicted=[its not clean]\n",
      "src=[er verlor ein buch], target=[he lost a book], predicted=[he has a book]\n",
      "src=[tom kam rein], target=[tom came in], predicted=[tom came in]\n",
      "src=[er ist ein dieb], target=[he is a thief], predicted=[he is a poet]\n",
      "src=[ich habe gesundigt], target=[i have sinned], predicted=[i got proof]\n",
      "src=[lass uns in frieden], target=[leave us alone], predicted=[lets us risks]\n",
      "src=[es ist jetzt an der zeit], target=[now is the time], predicted=[its time time time]\n",
      "src=[sie ist nicht jung], target=[she isnt young], predicted=[she isnt young]\n",
      "src=[tom rannte nach innen], target=[tom ran inside], predicted=[tom ran out]\n",
      "Bleu-1: 0.075459\n",
      "Bleu-2: 0.209155\n",
      "Bleu-3: 0.285918\n",
      "Bleu-4: 0.252777\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array, argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines\n",
    "):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# Map an integer to a word\n",
    "def map_int_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# Predict the target sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    pred = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in pred]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = map_int_to_word(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, tokenizer, source, raw_dataset):\n",
    "    predicted, actual = list(), list()\n",
    "    for i, source in enumerate(source):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, tokenizer, source)\n",
    "        raw_target, raw_source = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_source, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "\n",
    "    # Bleu Scores\n",
    "    print('Bleu-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0),smoothing_function=chencherry.method4))\n",
    "    print('Bleu-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method4))\n",
    "    print('Bleu-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=chencherry.method4))\n",
    "    print('Bleu-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=chencherry.method4))\n",
    "\n",
    "# Load datasets\n",
    "dataset = load_dataset('english-german-both.pkl')\n",
    "train = load_dataset('english-german-train.pkl')\n",
    "test = load_dataset('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "# Prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "print('Testing on trained examples')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "\n",
    "print('Testing on test examples')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
